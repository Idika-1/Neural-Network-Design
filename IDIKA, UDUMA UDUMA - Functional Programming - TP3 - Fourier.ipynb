{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae62a04d-8ce8-49f8-b258-5b35fd8f5745",
   "metadata": {},
   "source": [
    "__<h3>Author: IDIKA, UDUMA UDUMA</h3>__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02954f-e854-4094-93d2-3cbe5f71e86f",
   "metadata": {},
   "source": [
    "__<h3>Major: Data Management and Artificial Intelligence</h3>__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574793ae-b90f-42e9-9941-02c8774ba0a3",
   "metadata": {},
   "source": [
    "__<h3>Topic: Building a Neural Network from Scratch using JAX</h3>__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fded2800-a868-452c-b5d6-eb7af0353276",
   "metadata": {},
   "source": [
    "__<h3>Objective:</h3>__\n",
    "\n",
    "**Development of a Two-Layer Classification Neural Network from Scratch using JAX.**\n",
    "\n",
    "I achieved this via the following steps:\n",
    "\n",
    "Step 1: Defining the Neural Network Architecture  \n",
    "Step 2: Initializing Weights and Biases <br>\n",
    "Step 3: Implementing the Forward Pass <br>\n",
    "Step 4: Defining the Loss Function <br>\n",
    "Step 5: Implementing the Backpropagation Algorithm <br>\n",
    "Step 6: Updating Model Parameters (Weights and Biases) using Gradient Descent <br>\n",
    "Step 7: Training Loop Implementation <br>\n",
    "Step 8: Evaluating Model Performance <br>\n",
    "Step 9: Evaluating Model Accuracy on Test Set <br>\n",
    "\n",
    "In addition, I developed a **Fourier Basis function** as a function approximation model for features transformation \n",
    "before feeding the features through a neural network architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c618dae7-f2e3-4416-af5e-cd8bb3755aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import jax.random as random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf4e139-9b52-4a57-970c-15d5b9f432b1",
   "metadata": {},
   "source": [
    "**<h3>Part A: Development of a Fourier Basis Function for Features Transformation</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbb4474f-dfcb-4f10-a36e-3f0aeb8de7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of 'feature' train set: (7352, 5611)\n",
      "Shape of'feature' test set: (2947, 5611)\n",
      "\n",
      "Shape of 'target' train set: (7352,)\n",
      "Shape of'target' test set: (2947,)\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Understanding and Defining the  Dataset of UCI Human Activity Recognition (HAR) Using Smartphones\n",
    "\n",
    "#Data Source: UCI Machine Learning Repository - https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones\n",
    "\n",
    "\n",
    "#Features:\n",
    "# - 561 features present\n",
    "# - They are time-series signals from smartphone accelerometers and gyroscopes\n",
    "\n",
    "#Target:\n",
    "#The target has 6 classes\n",
    "# Each class represents one of the six available classes present (walking, walking upstairs, walking downstairs, sitting, standing, laying).\n",
    "\n",
    "\n",
    "# Load feature names\n",
    "feature_names_df = pd.read_csv(\"features.txt\", sep=r\"\\s+\", header=None, names=[\"index\", \"feature_name\"])\n",
    "\n",
    "# Ensure unique feature names by appending an index to duplicates\n",
    "feature_names_df[\"feature_name\"] = feature_names_df[\"feature_name\"].astype(str)\n",
    "feature_names = pd.Series(feature_names_df[\"feature_name\"]).rename_axis(\"idx\").reset_index()\n",
    "\n",
    "# Resolve duplicates by appending a count index\n",
    "feature_names[\"unique_name\"] = feature_names.groupby(\"feature_name\")[\"idx\"].rank(method=\"first\", ascending=True).astype(int)\n",
    "feature_names[\"feature_name\"] = feature_names.apply(lambda x: f\"{x['feature_name']}_{x['unique_name']}\" if x[\"unique_name\"] > 1 else x[\"feature_name\"], axis=1)\n",
    "\n",
    "# Convert to numpy array\n",
    "feature_names = feature_names[\"feature_name\"].values\n",
    "\n",
    "# Load training features\n",
    "X_train = jnp.array(pd.read_csv(\"train/X_train.txt\", sep=r\"\\s+\", header=None, names=feature_names))\n",
    "\n",
    "# Load training labels\n",
    "y_train = jnp.array(pd.read_csv(\"train/y_train.txt\", header=None, names=[\"Activity\"]).values).squeeze()\n",
    "\n",
    "# Load test features\n",
    "X_test = jnp.array(pd.read_csv(\"test/X_test.txt\", sep=r\"\\s+\", header=None, names=feature_names))\n",
    "\n",
    "# Load test labels\n",
    "y_test = jnp.array(pd.read_csv(\"test/y_test.txt\", header=None, names=[\"Activity\"]).values).squeeze()\n",
    "\n",
    "#Step 2:\n",
    "#Generating the Fourier Features\n",
    "\n",
    "#First, we write a function that generates the Fourier Basis features for a given input, 'X' and M\n",
    "M = 5\n",
    "\n",
    "def fourier_basis(x, m):\n",
    "    if x.ndim == 1:\n",
    "        x_column = x.reshape(-1,1)            #Ensuring the input is a column vector\n",
    "        k = jnp.arange(1, m+1).reshape(-1,1)  #Creating an array of frequencies\n",
    "    \n",
    "        ones_term = jnp.ones_like(x_column)              #Bias term. Shape: (n,1) where n is the number of elements. (50,1) in this case\n",
    "        sine_terms = (jnp.sin(2 * jnp.pi * k.T * x_column))    #Broadcast between k.T => (1,m) and x_column => (n,1). Final Shape: (n,m) i.e. (50, 5) in this case\n",
    "        cosine_terms = (jnp.cos(2 * jnp.pi * k.T * x_column))\n",
    "        fourier_features = jnp.column_stack((ones_term, sine_terms, cosine_terms))\n",
    "    \n",
    "    if x.ndim == 2:\n",
    "        n_samples, n_features = x.shape  # Number of samples and features\n",
    "        k = jnp.arange(1, m + 1).reshape(-1, 1)  # Frequencies\n",
    "\n",
    "        # Initialize the list to hold all Fourier terms\n",
    "        fourier_terms = [jnp.ones((n_samples, 1))]  # Start with the bias term\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            X = x[:, feature].reshape(-1, 1)  # Select the current feature\n",
    "            # Sine and cosine terms for the current feature\n",
    "            sine_terms = jnp.sin(2 * jnp.pi * k.T * X)  # Shape: (n_samples, m)\n",
    "            cosine_terms = jnp.cos(2 * jnp.pi * k.T * X)  # Shape: (n_samples, m)\n",
    "\n",
    "            # Append the sine and cosine terms to the list\n",
    "            fourier_terms.append(sine_terms)\n",
    "            fourier_terms.append(cosine_terms)\n",
    "\n",
    "        # Concatenate all terms along the second axis\n",
    "        fourier_features = jnp.hstack(fourier_terms)\n",
    "  \n",
    "    return fourier_features\n",
    "\n",
    "x_fourier_train = fourier_basis(X_train, M)\n",
    "x_fourier_test = fourier_basis(X_test, M)\n",
    "\n",
    "#Printing the shapes of the train and test features\n",
    "print(f\"Shape of 'feature' train set: {x_fourier_train.shape}\")\n",
    "print(f\"Shape of'feature' test set: {x_fourier_test.shape}\\n\")\n",
    "\n",
    "print(f\"Shape of 'target' train set: {y_train.shape}\")\n",
    "print(f\"Shape of'target' test set: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ac0092-a90a-457f-b1ee-6d0c463f86cc",
   "metadata": {},
   "source": [
    "**<h3>Part B: Development of a Neural Network Model</h3>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a38b7-078f-4d8c-9a49-781ad855421b",
   "metadata": {},
   "source": [
    "**Note: This Neural Network Implementation is for a 2-layer neural network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e518867-51dd-4b06-ae8d-df1f7948f286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n",
      "W1: (5611, 400)\n",
      "b1: (400,)\n",
      "W2: (400, 6)\n",
      "b2: (6,)\n",
      "\n",
      "Initial Model Parameters:\n",
      "W1: [[ 0.00568704  0.03380947  0.02237879 ...  0.01826228 -0.03240708\n",
      "   0.00025938]\n",
      " [ 0.04217771 -0.03270132  0.00511352 ... -0.01251905  0.0012203\n",
      "   0.00826543]\n",
      " [ 0.01441404  0.02760401  0.03661674 ...  0.02681212  0.02742296\n",
      "   0.01426434]\n",
      " ...\n",
      " [-0.00027428 -0.0239825   0.03206769 ... -0.01241341 -0.00484662\n",
      "   0.0088717 ]\n",
      " [ 0.01042465  0.01452601  0.00502385 ...  0.00403174 -0.0031096\n",
      "   0.00354301]\n",
      " [-0.00153755  0.01251395  0.05140643 ...  0.03141801 -0.00955447\n",
      "  -0.03263802]]\n",
      "\n",
      "b1: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "W2: [[ 0.09605838  0.03873228 -0.02330294 -0.0254774   0.06316271  0.01878332]\n",
      " [-0.02655715 -0.01473401 -0.07900513  0.10853028  0.04018363  0.06284911]\n",
      " [ 0.05663508 -0.01055389 -0.01767438 -0.04586948  0.06414918  0.03655863]\n",
      " ...\n",
      " [-0.07008652  0.04824203 -0.11664241  0.12523519 -0.0718319   0.08028391]\n",
      " [-0.03583057 -0.05511836 -0.07045225 -0.00866567 -0.05862925 -0.01981416]\n",
      " [-0.05263691  0.03165275  0.00752276 -0.02677782 -0.1059152  -0.18445179]]\n",
      "\n",
      "b2: [0. 0. 0. 0. 0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step 1:\n",
    "#Define an initialization function for the weights\n",
    "\n",
    "def initialization_fn(key, shape):\n",
    "        return random.normal(key, shape) * jnp.sqrt(2/shape[0])   #He initialization for ReLU activation\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, key, input_dim, hidden_dim, output_dim):\n",
    "        #Define the parameters of the network\n",
    "        key1, key2 = random.split(key, num=2)\n",
    "        \n",
    "        self.W1 = initialization_fn(key1, (input_dim, hidden_dim))\n",
    "        self.b1 = jnp.zeros(hidden_dim,)                                #biases are initialized to zero\n",
    "\n",
    "        self.W2 = initialization_fn(key2, (hidden_dim, output_dim))\n",
    "        self.b2 = jnp.zeros(output_dim,)\n",
    "\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        z1 = jnp.dot(x, self.W1) + self.b1     #pre-activation of the hidden layer\n",
    "        h = jax.nn.relu(z1)                    #applying ReLU activation for the hidden layer\n",
    "\n",
    "        z2 = jnp.dot(h, self.W2) + self.b2     #pre-activation of the output layer\n",
    "        y_pred = jax.nn.softmax(z2)            #applying Softmax to get the probabilities of the predicted target\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "x = x_fourier_train\n",
    "y = y_train\n",
    "\n",
    "#Creating an instance of the NeuralNetwork class\n",
    "main_key = random.PRNGKey(42)\n",
    "input_dim = x.shape[1]\n",
    "hidden_dim = 400                              #Numboer of neurons in the hidden layer\n",
    "output_dim = jnp.unique(y_train).size         #Number of neurons in the output layer corresponds to the number of distinct classes in the target\n",
    "\n",
    "nn = NeuralNetwork(key=main_key, input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)  #The _dim here refers to the number of neurons in each layer\n",
    "\n",
    "#Wrapping the parameters in a \"params\" dictionary\n",
    "params = {\n",
    "    \"W1\": nn.W1,\n",
    "    \"b1\": nn.b1,\n",
    "    \"W2\": nn.W2,\n",
    "    \"b2\": nn.b2\n",
    "}\n",
    "\n",
    "\n",
    "# Convert the target to One-Hot Encoding to obtain y_true\n",
    "num_classes = jnp.unique(y_train).size\n",
    "y_true = jnp.eye(num_classes)[y_train]\n",
    "\n",
    "#Determine initial y_pred from the initial weights and bias initialization\n",
    "y_pred = nn.forward_pass(x)\n",
    "\n",
    "#Printing the shapes of the model's parameters\n",
    "print(\"\\nModel Architecture:\")\n",
    "for k,v in params.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "\n",
    "\n",
    "#Defining the loss function (cross-entropy)\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    return -jnp.mean(jnp.sum(y_true * jnp.log(y_pred + 1e-8), axis=1))   #Added 1e-8 to avoid log(0) that causes instability\n",
    "\n",
    "#Defining the loss function wrapper to compute the gradient using the loss function\n",
    "def loss_func(params, x, y_true):\n",
    "    W1, b1, W2, b2 = params[\"W1\"], params[\"b1\"], params[\"W2\"], params[\"b2\"]\n",
    "    \n",
    "    z1 = jnp.dot(x, W1) + b1     #pre-activation of the hidden layer\n",
    "    h = jax.nn.relu(z1)          #applying ReLU activation for the hidden layer\n",
    "\n",
    "    z2 = jnp.dot(h, W2) + b2     #pre-activation of the output layer\n",
    "    y_pred = jax.nn.softmax(z2)\n",
    "    \n",
    "    return cross_entropy_loss(y_true, y_pred)\n",
    "\n",
    "#Automatically determining the gradients during backpropagation\n",
    "grads = grad(loss_func)(params, x, y_true)\n",
    "\n",
    "#Printing the initial values of the Model's parameters (params)\n",
    "print(\"\\nInitial Model Parameters:\")\n",
    "for k, v in params.items():\n",
    "    print(f\"{k}: {v}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387468fe-0c41-4304-885f-dcd8713a3a67",
   "metadata": {},
   "source": [
    "**<h3>Model Parameters (Weights & Biases) Update using Gradient Descent  </h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fba1920-b68f-4d98-aaa0-3098ddedcc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Model Parameters:\n",
      "W1: [[ 0.00568704  0.03380947  0.02237879 ...  0.01826228 -0.03240708\n",
      "   0.00025938]\n",
      " [ 0.04217771 -0.03270132  0.00511352 ... -0.01251905  0.0012203\n",
      "   0.00826543]\n",
      " [ 0.01441404  0.02760401  0.03661674 ...  0.02681212  0.02742296\n",
      "   0.01426434]\n",
      " ...\n",
      " [-0.00027428 -0.0239825   0.03206769 ... -0.01241341 -0.00484662\n",
      "   0.0088717 ]\n",
      " [ 0.01042465  0.01452601  0.00502385 ...  0.00403174 -0.0031096\n",
      "   0.00354301]\n",
      " [-0.00153755  0.01251395  0.05140643 ...  0.03141801 -0.00955447\n",
      "  -0.03263802]]\n",
      "\n",
      "b1: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "W2: [[ 0.09605838  0.03873228 -0.02330294 -0.0254774   0.06316271  0.01878332]\n",
      " [-0.02655715 -0.01473401 -0.07900513  0.10853028  0.04018363  0.06284911]\n",
      " [ 0.05663508 -0.01055389 -0.01767438 -0.04586948  0.06414918  0.03655863]\n",
      " ...\n",
      " [-0.07008652  0.04824203 -0.11664241  0.12523519 -0.0718319   0.08028391]\n",
      " [-0.03583057 -0.05511836 -0.07045225 -0.00866567 -0.05862925 -0.01981416]\n",
      " [-0.05263691  0.03165275  0.00752276 -0.02677782 -0.1059152  -0.18445179]]\n",
      "\n",
      "b2: [0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "Updated Model Parameters:\n",
      "W1: [[ 5.62511245e-03  3.39808017e-02  2.23356150e-02 ...  1.82718150e-02\n",
      "  -3.23960669e-02 -7.49876781e-05]\n",
      " [ 4.21283133e-02 -3.25471610e-02  5.07871574e-03 ... -1.25152953e-02\n",
      "   1.23014429e-03  7.93879200e-03]\n",
      " [ 1.44238304e-02  2.75425371e-02  3.66227478e-02 ...  2.68048700e-02\n",
      "   2.74174083e-02  1.43826492e-02]\n",
      " ...\n",
      " [-2.93679739e-04 -2.39703115e-02  3.20518985e-02 ... -1.23976152e-02\n",
      "  -4.84236237e-03  8.83361325e-03]\n",
      " [ 1.04088755e-02  1.45365708e-02  5.01243165e-03 ...  4.05725604e-03\n",
      "  -3.10384668e-03  3.50282108e-03]\n",
      " [-1.54333108e-03  1.25126513e-02  5.14013059e-02 ...  3.14385965e-02\n",
      "  -9.55089834e-03 -3.26534957e-02]]\n",
      "\n",
      "W2: [[ 0.09548362  0.03889392 -0.02317496 -0.02531483  0.06311996  0.01894863]\n",
      " [-0.02787764 -0.01678752 -0.07946769  0.10679136  0.04165351  0.06695472]\n",
      " [ 0.05596922 -0.01057286 -0.01739705 -0.04546156  0.06404987  0.03665753]\n",
      " ...\n",
      " [-0.07048207  0.04759861 -0.11655134  0.12493895 -0.07156582  0.08126197]\n",
      " [-0.03615331 -0.05561904 -0.07038372 -0.00908552 -0.05839114 -0.01887754]\n",
      " [-0.05302785  0.03123364  0.00755836 -0.02710407 -0.10561369 -0.18365261]]\n",
      "\n",
      "b1: [-6.19234343e-05  1.71330888e-04 -4.31754524e-05 -1.67348160e-04\n",
      "  2.51714286e-04 -6.99487573e-05  3.69586196e-05  6.60285004e-05\n",
      "  7.98138790e-05 -5.63054753e-04  2.85050392e-05  1.78889735e-04\n",
      "  3.50234877e-05 -1.73701337e-04  3.14310068e-06 -1.00942907e-06\n",
      " -7.47449740e-05 -2.26109245e-04  1.39461976e-04  6.17515907e-05\n",
      " -1.90350311e-05 -7.58150691e-06 -6.38558486e-05  2.25365162e-04\n",
      "  7.78589965e-05 -9.98659816e-05  4.39074101e-05  7.92962783e-06\n",
      " -4.64010227e-05 -9.17759899e-05 -1.73220178e-04  6.72906172e-05\n",
      " -4.25582293e-05  1.33611748e-05  1.57013183e-05 -2.59250373e-04\n",
      " -1.90194514e-05  5.05467870e-05  2.88301148e-04 -3.34713004e-05\n",
      "  2.25649026e-04 -3.09219286e-05 -8.48490527e-05  2.13817300e-04\n",
      " -2.69706288e-05 -2.47923452e-07 -1.31052613e-04 -5.93433906e-05\n",
      " -2.03520794e-05 -4.12187146e-05  6.26434485e-05 -4.96666689e-05\n",
      " -6.17693004e-05 -3.06964102e-05 -2.81761808e-04  1.30972251e-06\n",
      "  2.45247629e-05  4.19377357e-05  1.01207370e-04 -1.79558956e-05\n",
      "  5.04749296e-05 -1.39717507e-04  4.67796053e-06  9.17747893e-05\n",
      " -1.08499935e-05 -9.92262903e-06 -4.09026252e-05 -1.45147365e-04\n",
      " -2.06897588e-04  4.58171999e-05 -2.80123149e-05  8.03071234e-05\n",
      "  1.39076801e-04  1.26257350e-04  4.80625531e-05 -1.64407524e-04\n",
      "  3.35472250e-05 -1.58115781e-05  2.12956161e-06  7.28368032e-05\n",
      " -1.98921371e-05  2.30132180e-04 -1.08503074e-04 -3.38342157e-04\n",
      " -1.46986553e-04  1.01569989e-04  1.52342500e-05 -3.09432144e-05\n",
      " -3.07993832e-05  4.33041860e-05 -5.03039009e-05  9.89671898e-05\n",
      " -2.35233994e-04 -2.36785345e-05 -6.88284417e-05 -2.06888926e-05\n",
      " -1.67750441e-05 -1.83455249e-05 -1.51825865e-04  1.79259208e-04\n",
      " -2.62837129e-04  1.22291705e-04 -7.20586177e-05 -9.98043834e-05\n",
      " -4.68502294e-05  8.29777218e-06 -1.54778463e-04 -8.39861314e-05\n",
      "  8.63418245e-05 -2.29036130e-04  7.54989960e-05  3.35953635e-04\n",
      " -9.40874015e-05 -2.43061440e-04 -2.82726305e-05 -5.19273126e-05\n",
      "  9.41791077e-05  4.38820243e-05  1.70170097e-04  2.77429004e-04\n",
      " -1.02628394e-04  4.27765663e-05 -1.04059349e-04 -1.68022394e-04\n",
      "  6.04914967e-05  4.79792607e-06  1.73599430e-04  3.48470843e-04\n",
      "  5.32537051e-05 -2.18077388e-04 -9.09355294e-05  7.48420134e-05\n",
      " -3.97281947e-05  2.34499057e-05  1.32579880e-04 -7.71348932e-05\n",
      "  3.33744218e-04 -3.84581981e-05 -1.52380089e-05  1.45346057e-04\n",
      "  1.14203314e-04  1.53936591e-04  3.53432290e-04 -1.00059435e-04\n",
      "  1.07160202e-04 -8.64535396e-05  5.64168795e-06 -3.59947560e-04\n",
      " -2.92581899e-06  4.02370752e-05  9.27850215e-06  5.75762242e-05\n",
      " -8.75190890e-05 -4.18010095e-05 -3.51106542e-06 -2.71938570e-05\n",
      " -3.22632404e-05  5.73585203e-05 -3.34927201e-04 -6.54679679e-05\n",
      "  1.88511549e-05 -2.65415292e-04 -2.31806389e-05 -2.50528228e-05\n",
      "  6.42632003e-05  3.87639884e-05 -3.29186582e-06  7.24201527e-05\n",
      " -3.03289326e-05 -6.94851915e-05  1.07500433e-04  2.60964764e-04\n",
      "  1.45557962e-04  3.79539488e-05  5.78583058e-05  2.21759008e-04\n",
      " -1.29562744e-04 -1.42390112e-04  9.37136429e-05  2.12037939e-05\n",
      " -4.46651484e-06 -5.70935044e-05 -1.43077137e-04  7.66022367e-06\n",
      "  6.02395448e-05  1.94376116e-05 -1.91612766e-04  5.43445276e-06\n",
      "  4.65915800e-05 -9.86041487e-05 -6.13658267e-05  1.00347046e-04\n",
      " -2.30879978e-05  2.83659901e-05 -1.72542001e-04 -1.50700389e-05\n",
      "  3.53068492e-04  1.08852437e-04  7.51613261e-05  1.93739077e-04\n",
      " -7.79015954e-06  4.23570928e-05 -1.89477680e-04 -1.19614262e-04\n",
      "  1.66305312e-04  3.11502362e-07 -9.29001399e-05 -1.38222706e-04\n",
      " -5.93614059e-05  1.09751549e-04 -4.51298867e-04 -1.46140548e-04\n",
      "  1.26304614e-04  4.83517142e-05 -2.04837168e-04 -8.83036555e-05\n",
      "  1.09133398e-04  7.92854989e-05 -1.76312664e-04  3.79102166e-05\n",
      "  3.65559099e-05  1.01090191e-04 -1.11708003e-04  4.88055121e-05\n",
      " -1.07757522e-04 -3.91658075e-04  1.30191038e-04  1.85152743e-07\n",
      " -8.13963343e-05 -2.57240185e-06 -1.89073719e-04  2.87480343e-05\n",
      "  3.35787772e-05 -1.11587266e-04 -3.97020049e-04 -5.16800574e-05\n",
      "  4.41523989e-05  2.53876133e-05  8.38902924e-05 -2.66646002e-05\n",
      " -1.01285274e-04  3.50569499e-05  2.41293237e-05 -4.75663967e-07\n",
      " -1.69483959e-04  6.96907344e-04 -1.48943800e-04 -4.32579545e-05\n",
      "  9.75451258e-05  1.42702236e-04 -4.05367755e-04 -6.79529549e-05\n",
      " -2.84085429e-04  8.45251925e-05 -5.86314243e-04  1.08293571e-04\n",
      " -1.67092567e-05 -1.10651905e-04 -1.31859138e-04 -2.81917964e-05\n",
      " -1.04292092e-04 -9.50468384e-05 -6.65809603e-06 -3.97691110e-05\n",
      " -1.74651665e-04 -1.02133359e-04  1.90866776e-04  4.78369438e-05\n",
      " -1.30166329e-04 -3.33540156e-05 -2.48611701e-04 -2.56425905e-04\n",
      " -7.94258303e-05 -2.21891532e-05  3.08865128e-04  7.13636633e-04\n",
      "  2.68283020e-05 -1.06461717e-04  1.29782893e-05 -1.41706201e-04\n",
      "  5.84654153e-05  2.05502547e-05  5.15568136e-05 -2.78699648e-04\n",
      "  1.95697739e-05 -1.09800771e-04 -9.17228463e-05  2.65106719e-05\n",
      "  1.19148019e-04  8.24491071e-05 -8.84105521e-06 -1.62762924e-04\n",
      " -1.33679670e-04  1.40673088e-04 -8.08070763e-05  2.23753377e-04\n",
      " -1.23233607e-04  8.11259888e-05  1.58120528e-07  2.09759164e-04\n",
      " -1.82341042e-04 -4.82790056e-05 -8.24986841e-04 -8.08803088e-06\n",
      " -4.31165281e-05  1.32082991e-04  2.17910238e-05  1.30813220e-04\n",
      "  1.61177508e-04 -3.93015507e-05  5.07546865e-05 -8.95797621e-06\n",
      " -1.56633323e-04  2.39460733e-05  7.49196215e-06  5.08592420e-05\n",
      " -1.01699792e-04 -6.41762017e-05 -8.14470623e-05 -2.88832980e-05\n",
      "  2.60192086e-04 -3.26625704e-05 -2.70266552e-04 -1.03467028e-05\n",
      "  1.71526062e-05 -2.89699441e-04 -4.05000683e-05  1.23234524e-04\n",
      " -1.87498608e-05  1.19688229e-04 -5.62868190e-05  3.92277725e-05\n",
      " -6.98462172e-05 -2.79636559e-04 -1.70639876e-04  1.75680223e-04\n",
      " -2.21554932e-04 -2.55691648e-05  1.47795299e-06 -1.49470943e-04\n",
      "  6.45354739e-05 -9.94156107e-06  4.69196857e-05  1.01512203e-04\n",
      "  7.15619026e-05  3.11846961e-05 -3.12824232e-05 -3.12870980e-05\n",
      "  1.56707261e-04  2.28458339e-05  8.88845898e-05 -9.71791596e-05\n",
      " -5.48178559e-06 -4.14209680e-05  1.69488005e-04  1.37148891e-04\n",
      " -3.04843834e-05  2.76342849e-04 -2.13227722e-05  2.60788802e-05\n",
      "  6.61587546e-05 -1.45814382e-04 -1.16760384e-05 -1.06830666e-04\n",
      "  5.64321708e-05 -1.05526946e-04 -5.67705902e-05 -4.78904531e-06\n",
      " -3.05831709e-05  2.39790224e-05  6.58798017e-05 -2.07819670e-04\n",
      " -9.77554737e-05  1.54104168e-06 -4.87166872e-05 -2.26067114e-05\n",
      " -2.25592172e-04 -4.29839245e-04 -2.94001598e-04  2.21966202e-05\n",
      "  6.63427127e-05 -1.92819061e-04 -1.24269209e-04 -1.19454082e-04\n",
      " -2.59048684e-04  2.31384547e-05 -3.09664727e-04 -1.36252231e-04\n",
      " -9.61211190e-05  8.06918106e-05 -9.65071449e-05 -2.51632046e-05\n",
      "  2.11564875e-05 -6.97346768e-05  1.08397609e-04 -1.34874936e-04\n",
      " -3.74420451e-05  9.53279323e-06  1.10131177e-05 -3.34371027e-04]\n",
      "\n",
      "b2: [-0.00161156 -0.00155498  0.00036041 -0.00101379  0.00097373  0.00284619]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Since JAX is designed for functional programming, \n",
    "#we will return updated parameters instead of modifying parameters in place\n",
    "\n",
    "learning_rate=0.01\n",
    "\n",
    "def update_params(params, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs a gradient descent update on the neural network parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    - params: Dictionary containing the initial model parameters (W1, b1, W2, b2).\n",
    "    - gradients: Gradient function output from JAX, same structure as params.\n",
    "    - learning_rate: Step size for parameter updates.\n",
    "    \n",
    "    Returns:\n",
    "    - Updated parameters.\n",
    "    \"\"\"\n",
    "    updated_params = jax.tree_util.tree_map(lambda p, g: p - learning_rate * g, params, gradients)\n",
    "    return updated_params\n",
    "\n",
    "#Testing the function using our existing sample data\n",
    "\n",
    "updated_params = update_params(params, grads, learning_rate)\n",
    "\n",
    "#Printing the initial values of the Model's parameters (params)\n",
    "print(\"Initial Model Parameters:\")\n",
    "for k, v in params.items():\n",
    "    print(f\"{k}: {v}\\n\")\n",
    "\n",
    "#Printing the values of the model's updated_parameters\n",
    "print(\"\\nUpdated Model Parameters:\")\n",
    "for k, v in updated_params.items():\n",
    "    print(f\"{k}: {v}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6676cf-f6b4-4630-8d68-cb1d14beecd6",
   "metadata": {},
   "source": [
    "**<h3>Training Loop Implementation</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62854a37-5461-4257-8bef-8e631dd9504c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This entails structuring our training process to iteratively update the parameters over multiple epochs\n",
    "epochs = 200\n",
    "def train_model(params, x_train, y_train, epochs, learning_rate):\n",
    "    \"\"\"\n",
    "    Trains the neural network using gradient descent.\n",
    "\n",
    "    Arguments:\n",
    "    - params: Dictionary of initial model parameters.\n",
    "    - x_train: Input training data.\n",
    "    - y_train: True labels (one-hot encoded).\n",
    "    - epochs: Number of iterations for training.\n",
    "    - learning_rate: Step size for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "    - params: Trained parameters after `epochs` iterations.\n",
    "    - epoch_list: A list that contains some specified epochs\n",
    "    - loss_list: A list that contains the loss for for the specified epochs\n",
    "    \"\"\"\n",
    "    epoch_list = []\n",
    "    loss_list = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #Compute gradients\n",
    "        grads = grad(loss_func)(params, x_train, y_train)\n",
    "\n",
    "        #Update parameters\n",
    "        params = update_params(params, grads, learning_rate)\n",
    "\n",
    "        #Compute loss after 10 epochs for monitoring\n",
    "        if epoch % 10 == 0:\n",
    "            loss = loss_func(params, x_train, y_train)\n",
    "            #print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "\n",
    "            loss_value = float(loss)\n",
    "            epoch_list.append(epoch)\n",
    "            loss_list.append(round(loss_value, 4))\n",
    "\n",
    "    return params, epoch_list, loss_list\n",
    "        \n",
    "#Testing the training loop\n",
    "x_train = x.copy()\n",
    "y_train = y_true.copy()\n",
    "\n",
    "final_params, epoch_list, loss_list = train_model(params, x_train, y_train, epochs, learning_rate)\n",
    "\n",
    "#Printing the final model parameters\n",
    "#print(f\"\\nFinal Model Parameters:\")\n",
    "#for k, v in final_params.items():\n",
    "    #print(f\"{k}: {v}\\n\")\n",
    "\n",
    "#Visualizing the loss over the epoch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#print(loss_list)\n",
    "#print(epoch_list)\n",
    "plt.plot(epoch_list, loss_list)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbbc915-f86d-416d-971e-1254c08f58f6",
   "metadata": {},
   "source": [
    "**<h3>Evaluating Model Performance </h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d93880c4-452d-4b0e-b5c9-65bffd06bafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss: 0.1981\n",
      "\n",
      "Predicted Values:\n",
      "[[6.76441705e-04 4.77577996e-04 4.42522229e-04 8.93412915e-04\n",
      "  1.11843266e-01 8.85666907e-01]\n",
      " [1.33459864e-04 2.91111151e-04 1.40650838e-04 2.52805767e-04\n",
      "  3.88617694e-01 6.10564291e-01]\n",
      " [1.34364411e-04 3.31605377e-04 1.68751241e-04 5.15211490e-04\n",
      "  1.33023977e-01 8.65826011e-01]\n",
      " ...\n",
      " [2.09298105e-05 1.71310399e-02 9.79761600e-01 2.12004920e-03\n",
      "  4.32898087e-04 5.33493876e-04]\n",
      " [1.13297036e-04 1.68001540e-02 9.76582944e-01 5.23757283e-03\n",
      "  1.65393416e-04 1.10063876e-03]\n",
      " [4.95315704e-04 1.72996391e-02 9.76372302e-01 2.84421397e-03\n",
      "  1.90694438e-04 2.79785600e-03]]\n"
     ]
    }
   ],
   "source": [
    "#Compute the final loss of the model\n",
    "final_loss = loss_func(final_params, x_train, y_train)\n",
    "print(f\"Final Loss: {final_loss:.4f}\")\n",
    "\n",
    "#Implementing a predict function to make predictions using the final_params\n",
    "def predict(params, x):\n",
    "    z1 = jnp.dot(x, params[\"W1\"]) + params[\"b1\"]\n",
    "    h = jax.nn.relu(z1)\n",
    "\n",
    "    z2 = jnp.dot(h, params[\"W2\"]) + params[\"b2\"]\n",
    "    y_pred = jax.nn.softmax(z2)\n",
    "    return y_pred\n",
    "\n",
    "#Prediction on training data\n",
    "y_pred = predict(final_params, x_train)\n",
    "print(f\"\\nPredicted Values:\\n{y_pred}\")\n",
    "\n",
    "#Evaluating Model Performance\n",
    "def compute_accuracy(y_pred, y_true):\n",
    "    \"\"\"Compute accuracy by comparing predicted and actual labels\"\"\"\n",
    "    predicted_labels = jnp.argmax(y_pred, axis=1)  #Getting the class with the highest probability\n",
    "    actual_labels = jnp.argmax(y_true, axis=1)     #Getting the actual class labels\n",
    "    accuracy = jnp.mean(predicted_labels == actual_labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e2df72e-ddbc-4545-8354-c24683b0c8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Accuracy: 95.14%\n"
     ]
    }
   ],
   "source": [
    "#Computing the training accuracy\n",
    "accuracy = compute_accuracy(y_pred, y_train)\n",
    "print(f\"\\nTraining Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a1bd1c-8c26-415d-92be-08e11d01d3b5",
   "metadata": {},
   "source": [
    "**<h3>Evaluating Model Accuracy on the Test Set</h3>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05b16d2f-ba19-455d-8db6-5f562cf9161a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Accuracy: 86.94%\n"
     ]
    }
   ],
   "source": [
    "#Prediction on test data\n",
    "y_pred_test = predict(final_params, x_fourier_test)\n",
    "\n",
    "y_true_test = jnp.eye(num_classes)[y_test]   #One-hot encoded values of the y_test\n",
    "\n",
    "#Computing the test accuracy\n",
    "accuracy_test = compute_accuracy(y_pred_test, y_true_test)\n",
    "print(f\"\\nTesting Accuracy: {accuracy_test * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
